<div align="center">

# ğŸš— Vision-Enabled AI Co-Passenger

### Your intelligent in-car companion with eyes, voice, and personality

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Node.js](https://img.shields.io/badge/Node.js-16+-339933?logo=node.js&logoColor=white)](https://nodejs.org/)
[![Python](https://img.shields.io/badge/Python-3.8+-3776AB?logo=python&logoColor=white)](https://www.python.org/)
[![React](https://img.shields.io/badge/React-18.2-61DAFB?logo=react&logoColor=black)](https://react.dev/)
[![Three.js](https://img.shields.io/badge/Three.js-3D-000000?logo=three.js&logoColor=white)](https://threejs.org/)
[![MongoDB](https://img.shields.io/badge/MongoDB-Latest-47A248?logo=mongodb&logoColor=white)](https://www.mongodb.com/)
[![Gemini](https://img.shields.io/badge/Google-Gemini_AI-4285F4?logo=google&logoColor=white)](https://ai.google.dev/)
[![ElevenLabs](https://img.shields.io/badge/ElevenLabs-TTS-6366F1)](https://elevenlabs.io/)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](CONTRIBUTING.md)
[![Made with â¤](https://img.shields.io/badge/Made%20with-â¤-red.svg)](https://github.com)
[![Status](https://img.shields.io/badge/Status-Active-success)](https://github.com)

<p align="center">
  <img src="https://readme-typing-svg.demolab.com?font=Fira+Code&size=22&duration=3000&pause=1000&color=F75C7E&center=true&vCenter=true&width=600&lines=See+the+road+ahead...;Understand+your+surroundings...;Chat+naturally+with+AI...;Experience+the+future+of+driving!" alt="Typing SVG" />
</p>

---

[ğŸ¬ Watch Demo](#-preview--demo) â€¢ 
[âœ¨ Features](#-features) â€¢ 
[ğŸ— Architecture](#-architecture) â€¢ 
[ğŸš€ Quick Start](#-quick-start) â€¢ 
[ğŸ”Œ API Reference](#-api-reference) â€¢ 
[ğŸ¤ Contributing](#-contributing)

</div>

---

## ğŸ“– Table of Contents

- [ğŸ¬ Preview & Demo](#-preview--demo)
- [âœ¨ Features](#-features)
- [ğŸ§  Tech Stack](#-tech-stack)
- [ğŸ— Architecture](#-architecture)
- [ğŸ’¡ How It Works](#-how-it-works)
- [ğŸš€ Quick Start](#-quick-start)
- [ğŸ”Œ API Reference](#-api-reference)
- [ğŸ¤ Contributing](#-contributing)
- [ğŸ“„ License](#-license)
- [ğŸ™ Acknowledgments](#-acknowledgments)

---

## ğŸ¬ Preview & Demo

<details>
<summary>ğŸ¥ Click to view demo video</summary>

<video src="Car_Assistant_Record_final.mp4" controls width="720"></video>

</details>

### What You'll Experience

- ğŸ—£ *Natural Conversations* â€“ Talk to your AI co-passenger like a friend  
- ğŸ‘ *Visual Understanding* â€“ â€œLook aroundâ€ and get real-time scene descriptions  
- ğŸ­ *Expressive Avatar* â€“ Emotions & lip-sync come alive in 3D  
- ğŸ”Š *Lifelike Speech* â€“ Neural TTS with natural tone & timing  
- ğŸ—º *Smart Context* â€“ Navigation, weather, and awareness combined  

---

## âœ¨ Features

<table>
<tr>
<td width="50%">

### ğŸ¯ Core Capabilities

- âœ… *Multi-Modal AI*
  - Gemini Vision API for visual analysis  
  - Gemini Pro for natural conversation  
  - Real-time multi-sensor fusion  

- âœ… *3D Avatar System*
  - Emotion-based facial expressions  
  - Rhubarb-powered accurate lip-sync  
  - Real-time animation blending  

- âœ… *Speech & Audio*
  - ElevenLabs neural TTS  
  - Multi-language voices  
  - Emotion-adaptive tone  

</td>
<td width="50%">

### ğŸ›  Advanced Features

- âœ… *Authentication & Persistence*
  - JWT-secured sessions  
  - MongoDB chat history  
  - Persistent login  

- âœ… *Vision Triggers*
  - Keyword detection (â€œvisualizeâ€, â€œlook aroundâ€)  
  - Multi-frame analysis  
  - Context-aware replies  

- âœ… *Smart Integrations*
  - Google Maps + Weather APIs  
  - Object detection via TensorFlow.js  
  - Real-time camera feed  

</td>
</tr>
</table>

---

## ğŸ§  Tech Stack

<div align="center">

### âš› Frontend

[![React](https://img.shields.io/badge/React-61DAFB?style=for-the-badge&logo=react&logoColor=black)](https://react.dev/)
[![Vite](https://img.shields.io/badge/Vite-646CFF?style=for-the-badge&logo=vite&logoColor=white)](https://vitejs.dev/)
[![Three.js](https://img.shields.io/badge/Three.js-000000?style=for-the-badge&logo=three.js&logoColor=white)](https://threejs.org/)
[![TailwindCSS](https://img.shields.io/badge/TailwindCSS-38B2AC?style=for-the-badge&logo=tailwind-css&logoColor=white)](https://tailwindcss.com/)

### ğŸ§© Backend

[![Node.js](https://img.shields.io/badge/Node.js-339933?style=for-the-badge&logo=node.js&logoColor=white)](https://nodejs.org/)
[![Express](https://img.shields.io/badge/Express-000000?style=for-the-badge&logo=express&logoColor=white)](https://expressjs.com/)
[![MongoDB](https://img.shields.io/badge/MongoDB-47A248?style=for-the-badge&logo=mongodb&logoColor=white)](https://www.mongodb.com/)
[![JWT](https://img.shields.io/badge/JWT-000000?style=for-the-badge&logo=json-web-tokens&logoColor=white)](https://jwt.io/)

### ğŸ§  AI & Vision

[![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/)
[![Flask](https://img.shields.io/badge/Flask-000000?style=for-the-badge&logo=flask&logoColor=white)](https://flask.palletsprojects.com/)
[![OpenCV](https://img.shields.io/badge/OpenCV-5C3EE8?style=for-the-badge&logo=opencv&logoColor=white)](https://opencv.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)](https://pytorch.org/)
[![Gemini](https://img.shields.io/badge/Google_Gemini-4285F4?style=for-the-badge&logo=google&logoColor=white)](https://ai.google.dev/)
[![ElevenLabs](https://img.shields.io/badge/ElevenLabs-6366F1?style=for-the-badge)](https://elevenlabs.io/)

</div>

---

## ğŸ— Architecture

### ğŸ§© System Overview

mermaid
graph TB
    subgraph UI_Layer["ğŸ¨ User Interface Layer"]
        UI[React Frontend<br>Vite + Three.js]
        Avatar[3D Avatar<br>Animations & Lip-sync]
    end
    
    subgraph API_Layer["ğŸ”§ API Gateway Layer"]
        API[Express Backend<br>Node.js + JWT Auth]
        Auth[Authentication<br>Middleware]
    end
    
    subgraph AI_Layer["ğŸ§  AI Engine Layer"]
        Vision[Vision Service<br>Python Flask]
        Chat[Gemini Chat<br>Pro Model]
        VisionAI[Gemini Vision<br>Image Analysis]
        TTS[ElevenLabs TTS<br>Neural Speech]
        Lip[Rhubarb Lip-sync<br>Mouth Cues]
    end
    
    subgraph Data_Layer["ğŸ’¾ Data Layer"]
        DB[(MongoDB<br>User & Chat Data)]
        Frames[Frame Storage<br>Captured Images]
    end
    
    subgraph External_Layer["ğŸŒ External Services"]
        Maps[Google Maps API]
        Weather[Weather API]
        Camera[Phone Camera<br>IP Webcam]
    end
    
    UI --> API
    UI --> Avatar
    API --> Auth
    Auth --> DB
    API --> Chat
    API --> Vision
    API --> TTS
    API --> Lip
    Vision --> Camera
    Vision --> VisionAI
    Vision --> Frames
    Chat --> VisionAI
    UI --> Maps
    UI --> Weather
    
    style UI fill:#00bcd4,stroke:#ffffff,stroke-width:1.5px,color:#fff
    style Avatar fill:#26c6da,stroke:#ffffff,stroke-width:1.5px,color:#fff
    style API fill:#43a047,stroke:#ffffff,stroke-width:1.5px,color:#fff
    style Auth fill:#66bb6a,stroke:#ffffff,stroke-width:1.5px,color:#fff
    style Vision fill:#1e88e5,stroke:#ffffff,stroke-width:1.5px,color:#fff
    style Chat fill:#5c6bc0,stroke:#ffffff,stroke-width:1.5px,color:#fff
    style VisionAI fill:#3949ab,stroke:#ffffff,stroke-width:1.5px,color:#fff
    style TTS fill:#7e57c2,stroke:#ffffff,stroke-width:1.5px,color:#fff
    style Lip fill:#ab47bc,stroke:#ffffff,stroke-width:1.5px,color:#fff
    style DB fill:#2e7d32,stroke:#ffffff,stroke-width:1.5px,color:#fff
    style Frames fill:#388e3c,stroke:#ffffff,stroke-width:1.5px,color:#fff
    style Maps fill:#fbc02d,stroke:#000,stroke-width:1.5px,color:#000
    style Weather fill:#ffb300,stroke:#000,stroke-width:1.5px,color:#000
    style Camera fill:#ff7043,stroke:#000,stroke-width:1.5px,color:#000



---

##   Quick Start

### Prerequisites

- âœ… *Node.js* 16+ ([Download](https://nodejs.org/))
- âœ… *Python* 3.8+ ([Download](https://www.python.org/))
- âœ… *MongoDB* ([Install](https://www.mongodb.com/try/download/community))
- âœ… *FFmpeg* (for audio conversion)
- âœ… *Git* (optional)

### API Keys Required

| Service | Purpose | Get Key |
|---------|---------|---------|
| ğŸ¤– Google Gemini | Chat & Vision AI | [Get Key](https://makersuite.google.com/app/apikey) |
| ğŸ”Š ElevenLabs | Text-to-Speech | [Get Key](https://elevenlabs.io/) |
| ğŸ—º Google Maps (optional) | Navigation | [Get Key](https://developers.google.com/maps) |

### The Complete Interaction Pipeline

mermaid
flowchart TD
    Start([ğŸ‘¤ User Interaction]) --> Input{ğŸ¤ Input Type?}
    
    Input -->|ğŸ’¬ Text Message| CheckTrigger[ğŸ” Check for Vision Trigger]
    Input -->|ğŸ—£ Voice Command| STT[ğŸ™ Speech-to-Text]
    
    STT --> CheckTrigger
    
    CheckTrigger -->|ğŸ” Contains Trigger| VisionFlow[ğŸ‘ Vision Analysis Flow]
    CheckTrigger -->|ğŸ’¬ Normal Chat| ChatFlow[ğŸ¤– Standard Chat Flow]
    
    VisionFlow --> CaptureFrames[ğŸ“¹ Capture N Clear Frames]
    CaptureFrames --> QualityCheck{âœ… Quality OK?}
    QualityCheck -->|âœ… Yes| CombineFrames[ğŸ–¼ Combine into Grid Image]
    QualityCheck -->|âŒ No| CaptureFrames
    
    CombineFrames --> SaveImage[ğŸ’¾ Save to Disk]
    SaveImage --> GeminiVision[ğŸ¤– Gemini Vision Analysis]
    GeminiVision --> SceneDesc[ğŸ“ Scene Description Text]
    
    SceneDesc --> EnrichPrompt[âœ¨ Enrich with Context]
    ChatFlow --> EnrichPrompt
    
    EnrichPrompt --> GeminiChat[ğŸ§  Gemini Pro Chat]
    GeminiChat --> ParseResponse[ğŸ“‹ Parse JSON Response]
    
    ParseResponse --> GenerateAudio[ğŸ”Š ElevenLabs TTS]
    GenerateAudio --> GenerateLipsync[ğŸ‘„ Rhubarb Lip-sync]
    GenerateLipsync --> Package[ğŸ“¦ Package Message Data]
    
    Package --> SendToFrontend[ğŸ“¤ Send to Frontend]
    SendToFrontend --> RenderAvatar[ğŸ­ Render 3D Avatar]
    RenderAvatar --> PlayAudio[â–¶ Play Audio]
    PlayAudio --> SyncAnimation[ğŸ¬ Sync Animation + Lip-sync]
    SyncAnimation --> UpdateUI[ğŸ”„ Update Chat History]
    
    UpdateUI --> SaveToDB[(ğŸ’¾ Save to MongoDB)]
    SaveToDB --> End([ğŸ”„ Conversation Continues])
    
    style VisionFlow fill:#ff9800
    style GeminiVision fill:#4285F4
    style GeminiChat fill:#34A853
    style GenerateAudio fill:#6366F1
    style RenderAvatar fill:#61dafb


</div>

| Layer | Technologies |
|-------|-------------|
| *Frontend* | React, Vite, Three.js, TailwindCSS |
| *Backend* | Node.js, Express, JWT, Mongoose |
| *Vision* | Python, Flask, OpenCV, PyTorch |
| *AI* | Google Gemini (Pro + Vision) |
| *Speech* | ElevenLabs TTS, Rhubarb Lip-Sync |
| *Database* | MongoDB |

## ğŸš€ Quick Start

### 1) Install dependencies

powershell
pip install -r requirements-vision.txt
cd backend-gemini && npm install && cd ..
cd frontend && npm install && cd ..


### 2) Configure environment

Create a .env file (see backend-gemini/.env.example). Minimum variables:


GEMINI_API_KEY=your_gemini_api_key
ELEVEN_LABS_API_KEY=your_elevenlabs_api_key
MONGODB_URI=mongodb://localhost:27017/ai_chatbot_db


If you use a phone camera, set your video stream URL in two places:

- vision-service.py â†’ default for video_url
- frontend/src/components/HomeScreen/Screen.jsx â†’ requestBody.videoUrl

Android: IP Webcam (http://PHONE_IP:8080/video) â€¢ DroidCam (http://PHONE_IP:4747/video)

### 3) Start everything (Windows)

powershell
./start-all-services.ps1


This opens three terminals: Vision (5000), Backend (3000), Frontend (5173).

### 4) Login and chat

Open the frontend URL from the terminal output, sign up / sign in, and type messages like:

- â€œvisualize the environmentâ€
- â€œlook aroundâ€
- â€œwhat do you seeâ€

Youâ€™ll see an â€œAnalyzing surroundingsâ€¦â€ banner while frames are captured and analyzed. Then the avatar speaks a natural description with synced mouth cues and animation.

<!-- 
## Configuration (Advanced)

### Rhubarb Lipâ€‘Sync Path (Windows)
Update in backend-gemini/index.js if needed:
js
const rhubarbPath = "D:\\...\\backend-gemini\\bin\\Rhubarb-Lip-Sync-1.14.0-Windows\\rhubarb.exe";


### Vision Tuning
Edit vision-service.py:
- num_frames: number of frames to combine (default 4)
- frame_skip: capture stride (default 5)
- laplacian_threshold: quality check threshold

### Custom Trigger Keywords
Add in backend-gemini/index.js â†’ shouldTriggerVision()
-->

<!-- 
## API Reference

*Backend* (http://localhost:3000)
- POST /api/auth/signup â€” Create user
- POST /api/auth/login â€” Get JWT token
- POST /api/chat (JWT) â€” Send message: { message, videoUrl? }
- GET /api/chat-history (JWT) â€” List chat sessions

*Vision Service* (http://localhost:5000)
- GET /health â€” Health check
- POST /analyze-environment â€” Analyze video: { video_url }

Full API documentation in ARCHITECTURE.md
-->

---

## ğŸ”Œ API Reference

<details>
<summary>ğŸ“¡ Click to view API Endpoints</summary>

### Authentication Endpoints

#### POST /api/auth/signup
Create a new user account.

*Request:*
json
{
  "email": "user@example.com",
  "password": "securepassword123",
  "name": "John Doe"
}


*Response:*
json
{
  "token": "eyJhbGciOiJIUzI1NiIs...",
  "user": {
    "id": "64f1a714fe61576b46f27ca2",
    "email": "user@example.com",
    "name": "John Doe"
  }
}


---

#### POST /api/auth/login
Authenticate existing user.

*Request:*
json
{
  "email": "user@example.com",
  "password": "securepassword123"
}


### Backend API (http://localhost:3000)
| Method | Endpoint | Protection | Description |
|--------|----------|------------|-------------|
| POST | /api/auth/signup | Public | Creates a new user account. |
| POST | /api/auth/login | Public | Authenticates a user and returns a JWT. |
| POST | /api/chat | JWT | Sends a message for processing and gets a response. |
| GET | /api/chat-history | JWT | Retrieves the list of all chat sessions for the user. |
| GET | /api/chat-history/:id| JWT | Retrieves a specific chat session by its ID. |
| GET | /voices | Public | Fetches available TTS voices from ElevenLabs. |

### Vision Service API (http://localhost:5000)
| Method | Endpoint | Protection | Description |
|--------|----------|------------|-------------|
| GET | /health | Public | Checks if the vision service is running. |
| POST | /analyze-environment | Public | Receives a video URL, analyzes it, and returns a description. |

### Sample Chat Request/Response

*Request:*
json
{
  "message": "visualize the environment",
  "videoUrl": "http://192.168.1.100:8080/video"
}


*Response:*
json
{
  "messages": [
    {
      "text": "Looking around, I can see a tree-lined road...",
      "audio": "base64_encoded_audio_data",
      "lipsync": {
        "metadata": { ... },
        "mouthCues": [...]
      },
      "facialExpression": "smile",
      "animation": "Talking_1"
    }
  ]
}


</details>

---

## ğŸ¤ Contributing

We welcome contributions! Here's how you can help:

### Ways to Contribute

- ğŸ› *Report bugs* â†’ [Open an issue](https://github.com/yourusername/repo/issues)
- âœ¨ *Suggest features* â†’ [Feature request](https://github.com/yourusername/repo/issues/new)
- ğŸ“ *Improve docs* â†’ Submit a PR
- ğŸ§ª *Write tests* â†’ Add test coverage
- ğŸ¨ *Design assets* â†’ UI/UX improvements
- ğŸŒ *Translations* â†’ Multi-language support

### Development Workflow

1. *Fork* the repository
2. *Clone* your fork
3. *Create* a feature branch
4. *Make* your changes
5. *Test* thoroughly
6. *Commit* with clear messages
7. *Push* to your fork
8. *Open* a Pull Request

For detailed guidelines, see [CONTRIBUTING.md](CONTRIBUTING.md).

---

## ğŸ“„ License

This project is licensed under the *MIT License*.


MIT License

Copyright (c) 2025 AI Co-Passenger Contributors

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.


See [LICENSE](LICENSE) for full text.

---

## ğŸ™ Acknowledgments

### Technologies & Services

- ğŸ¤– *[Google Gemini](https://ai.google.dev/)* - Conversational AI & Vision understanding
- ğŸ”Š *[ElevenLabs](https://elevenlabs.io/)* - Neural text-to-speech
- ğŸ‘„ *[Rhubarb Lip Sync](https://github.com/DanielSWolf/rhubarb-lip-sync)* - Automatic mouth animation
- ğŸ¨ *[Three.js](https://threejs.org/)* - WebGL 3D rendering
- âš› *[React](https://react.dev/)* - UI framework
- âš¡ *[Vite](https://vitejs.dev/)* - Build tool
- ğŸ­ *[Mixamo](https://www.mixamo.com/)* - 3D character animations

### Special Thanks

- ğŸŒŸ All contributors and testers
- ğŸ’¬ The open-source community  
- ğŸ“ *HackCBS 8.0* Hackathon organizers
- â˜• Coffee for late-night coding sessions

---

<div align="center">

### ğŸ’– Made with love by *Keshav* and *Sanskar* for *Hack CBS 8.0* ğŸ†

Built during HackCBS 8.0 Hackathon, 2025

ğŸš— Happy driving with your intelligent coâ€‘passenger ğŸ‘ğŸ¤–

### ğŸ’« If you found this project helpful, please consider:

[![Star this repo](https://img.shields.io/github/stars/yourusername/repo?style=social)](https://github.com/yourusername/repo)
[![Follow on GitHub](https://img.shields.io/github/followers/yourusername?style=social)](https://github.com/yourusername)

*Made with â¤ and lots of â˜•*

[â¬† Back to Top](#-vision-enabled-ai-co-passenger)

</div>